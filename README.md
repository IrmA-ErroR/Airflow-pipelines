# Примеры из книги **Data Pipelines with Apache Airflow** *Bas Harenslak and Julian de Ruiter*

DAG (Directed Acyclic Graph / ориентированный ациклический граф) 
— это основная концепция Airflow, объединяющая задачи и организующая их с помощью зависимостей и отношений, указывающих, как они должны выполняться.

Концептуально алгоритм выполнения графа конвейера состоит из следующих шагов:
1. Для каждой открытой (= незавершенной) задачи в графе выполните следующие действия:
-  для каждого ребра, указывающего на задачу, проверьте, завершена ли «вышестоящая» задача на другом конце ребра;
-  если все вышестоящие задачи были выполнены, добавьте текущую задачу в *очередь* для выполнения.
2. Выполните задачи в очереди, помечая их как выполненные при завершении.
3. Вернитесь к шагу 1 и повторите действия, пока все задачи в графе не будут выполнены.

На высоком уровне Airflow состоит из трех основных компонентов:
- ***планировщик*** Airflow (Scheduler), чаще всего используется Celery – анализирует ОАГ(DAG), проверяет параметр 
schedule_interval и (если все в порядке) начинает планировать 
задачи ОАГ для выполнения, передавая их воркерам Airflow;
- воркеры (***workers***) Airflow – выбирают задачи, которые запланированы для выполнения, и выполняют их. Таким образом, они 
несут ответственность за фактическое «выполнение работы» (чаще всего встречается конфигурация с Celery);
-  ***веб-сервер*** Airflow – визуализирует ОАГ, анализируемые планировщиком, и предоставляет пользователям основной интерфейс 
для отслеживания выполнения графов и их результатов (Веб-приложение с панелью управления, написано на Flask)



## Подготовка окружения
Ubuntu 22.04.4 <br>
VSCodex64 1.89.1 <br>
Python 3.10.12 <br>

1. Создание виртуального окружения
```
python3 -m venv venv
which python  # /home/irina/airflow-pipelines/venv
```

2. Активация виртуального окружения
```
source venv/bin/activate # deactivate 

```

3. Установка Apache-Airflow
```
pip3 install apache-airflow
```


Apache Airflow свои настройки хранит в файле ```airflow.cfg```, который по умолчанию будет создан в домашней директории юзера по пути ```~/airflow/airflow.cfg```.
по умолчанию в качестве базы данных Airflow использует SQLite. 

Инициализация базы данных Airflow:
```bash 
airflow db init
```

Автоматически сгенерировался ```airflow.cfg``` с параметрами по умолчанию:

```python

import os

file_path = os.path.expanduser('~/airflow/airflow.cfg')

with open(file_path, 'r') as file:
    for line in file:
        if not line.strip().startswith('#'):
            # Выводим строку, если она не является комментарием
            print(line.strip())

```
