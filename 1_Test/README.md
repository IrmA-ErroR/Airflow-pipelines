# Тестовый запуск Apache Airflow 

## База
**Operator**

Оператор - это звено в цепочке задач. Используя оператор разработчик описывает какую задачу необходимо выполнить. В Airflow есть ряд готовых операторов, например:

- PythonOperator — оператор для исполнения python кода
- BashOperator — оператор для запуска bash скриптов/команд
- PostgresOperator — оператор для вызова SQL запросов в PostgreSQL БД
- RedshiftToS3Transfer — оператор для запуска UNLOAD команды из Redshift в S3
- EmailOperator — оператор для отправки электронных писем

**Sensor**

Сенсор - это разновидность Operator, его удобно использовать при реализации событийно ориентированных пайплайнов. В Airflow есть есть, например:

- PythonSensor — ждём, когда функция вернёт True
- S3Sensor — проверяет наличие объекта по ключу в S3-бакете
- RedisPubSubSensor — проверяет наличие сообщения в pub-sub очереди
- RedisKeySensor — проверяет существует ли переданный ключ в Redis хранилище
Чтобы создать свой сенсор, достаточно унаследоваться от BaseSensorOperator и переопределить метод poke.

**Hook**

Хуки это внешние интерфейсы для работы с различными сервисами: базы данных, внешние API ресурсы, распределенные хранилища типа S3, redis, memcached и т.д. Хуки являются строительными блоками операторов и берут на себя всю логику по взаимодействию с хранилищем конфигов и доступов.

## Описание
Тестовое пириложение

Postgresql<br>
Airflow:2.6.1<br>


docker-compose.yaml содержит несколько определений служб:

airflow-scheduler — планировщик отслеживает все задачи и группы DAG, а затем запускает экземпляры задач после завершения их зависимостей.

airflow-webserver — веб-сервер доступен по адресу http://0.0.0.0:8080.

airflow-worker — рабочий процесс, который выполняет задачи, заданные планировщиком.

airflow-triggerer — триггер запускает цикл событий для откладываемых задач.

airflow-init — служба инициализации.

postgres — база данных.

redis — брокер redis, который пересылает сообщения от планировщика к рабочему процессу.

Пример docker-compose.yml из официальной документации представлен в ```base_docker-compose.yml``` .

## Запуск 
Для запуска Docker контейнеров выполняем команду:

```bash
docker-compose up -d
```
Первый запуск занимает некоторое время, так как Docker загрузит необходимые образы.

Веб-интерфейс Airflow доступен по адресу http://0.0.0.0:8080. 

Вход в систему по умолчанию:

Логин: ```admin```<br>
Пароль: ```admin```<br>

В списке DAG'ов находим hello_world и включаем его, запускаем его кнопкой "Trigger DAG".
В логах должно появиться сообщение "Привет!".
*Проще всего увидеть результат в [логах таска](http://0.0.0.0:8080/dags/hello_world/grid?task_id=hello_task&dag_run_id=manual__2024-08-12T14%3A01%3A40.178526%2B00%3A00&tab=logs)*